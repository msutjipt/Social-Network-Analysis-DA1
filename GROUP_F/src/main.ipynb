{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load Dataset",
   "id": "349c570d9e944e1d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-18T13:00:52.495699Z",
     "start_time": "2025-01-18T13:00:52.144557Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"../data/dataset.json\"\n",
    "df = pd.read_json(file_path)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   timestamp                                               text     text_id  \\\n",
       "0 2024-10-31  Running a business means juggling countless ad...  2018569761   \n",
       "1 2024-10-31  Liz Truss is walking in the lingering shadow o...  2092717718   \n",
       "2 2024-10-31  The UK is bracing for war as government buildi...  2059143248   \n",
       "3 2024-10-31  Marrying a second or third cousin once removed...  2008209828   \n",
       "4 2024-10-31  It's truly disgraceful how the Indian National...  2001239278   \n",
       "\n",
       "               user     user_id  \n",
       "0     danielwoodard  1077866112  \n",
       "1  nelsonjacqueline  1089670430  \n",
       "2           ihooper  1007478642  \n",
       "3    wrightnicholas  1039258480  \n",
       "4         michael51  1021455936  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Running a business means juggling countless ad...</td>\n",
       "      <td>2018569761</td>\n",
       "      <td>danielwoodard</td>\n",
       "      <td>1077866112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Liz Truss is walking in the lingering shadow o...</td>\n",
       "      <td>2092717718</td>\n",
       "      <td>nelsonjacqueline</td>\n",
       "      <td>1089670430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>The UK is bracing for war as government buildi...</td>\n",
       "      <td>2059143248</td>\n",
       "      <td>ihooper</td>\n",
       "      <td>1007478642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Marrying a second or third cousin once removed...</td>\n",
       "      <td>2008209828</td>\n",
       "      <td>wrightnicholas</td>\n",
       "      <td>1039258480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>It's truly disgraceful how the Indian National...</td>\n",
       "      <td>2001239278</td>\n",
       "      <td>michael51</td>\n",
       "      <td>1021455936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Run Sentiment Analysis to Get Labels and Confidence Scores",
   "id": "540596eb56175304"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T21:52:36.698392Z",
     "start_time": "2025-01-18T13:38:16.500985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline with the Twitter-specific model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# Apply the pipeline to the 'text' column\n",
    "results = df['text'].apply(sentiment_pipeline)\n",
    "\n",
    "# Extract the label and score for each result and add them as new columns\n",
    "df['sentiment_label'] = results.apply(lambda x: x[0]['label'])\n",
    "df['sentiment_score'] = results.apply(lambda x: x[0]['score'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()\n"
   ],
   "id": "a8186c38db74e8f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/Library/Caches/pypoetry/virtualenvs/social-network-analysis-da1-KfAxiyL9-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   timestamp                                               text     text_id  \\\n",
       "0 2024-10-31  Running a business means juggling countless ad...  2018569761   \n",
       "1 2024-10-31  Liz Truss is walking in the lingering shadow o...  2092717718   \n",
       "2 2024-10-31  The UK is bracing for war as government buildi...  2059143248   \n",
       "3 2024-10-31  Marrying a second or third cousin once removed...  2008209828   \n",
       "4 2024-10-31  It's truly disgraceful how the Indian National...  2001239278   \n",
       "\n",
       "               user     user_id sentiment_label  sentiment_score  \n",
       "0     danielwoodard  1077866112        positive         0.781216  \n",
       "1  nelsonjacqueline  1089670430        negative         0.882852  \n",
       "2           ihooper  1007478642        negative         0.551017  \n",
       "3    wrightnicholas  1039258480        positive         0.747033  \n",
       "4         michael51  1021455936        negative         0.943839  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_id</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Running a business means juggling countless ad...</td>\n",
       "      <td>2018569761</td>\n",
       "      <td>danielwoodard</td>\n",
       "      <td>1077866112</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.781216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Liz Truss is walking in the lingering shadow o...</td>\n",
       "      <td>2092717718</td>\n",
       "      <td>nelsonjacqueline</td>\n",
       "      <td>1089670430</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.882852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>The UK is bracing for war as government buildi...</td>\n",
       "      <td>2059143248</td>\n",
       "      <td>ihooper</td>\n",
       "      <td>1007478642</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.551017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>Marrying a second or third cousin once removed...</td>\n",
       "      <td>2008209828</td>\n",
       "      <td>wrightnicholas</td>\n",
       "      <td>1039258480</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.747033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-10-31</td>\n",
       "      <td>It's truly disgraceful how the Indian National...</td>\n",
       "      <td>2001239278</td>\n",
       "      <td>michael51</td>\n",
       "      <td>1021455936</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.943839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T09:36:21.632261Z",
     "start_time": "2025-01-19T09:36:21.232504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = \"../data/sentiment_analysis_results.csv\"  # Specify your file name\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {output_csv_path}\")\n"
   ],
   "id": "2209b1494760773e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to ../data/sentiment_analysis_results.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Separate by Sentiment Label",
   "id": "d1d560654101c58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T11:01:39.905820Z",
     "start_time": "2025-01-19T11:01:39.893379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create separate lists for each sentiment\n",
    "positive_scores = df.loc[df['sentiment_label'] == 'positive', 'sentiment_score'].tolist()\n",
    "neutral_scores = df.loc[df['sentiment_label'] == 'neutral', 'sentiment_score'].tolist()\n",
    "negative_scores = df.loc[df['sentiment_label'] == 'negative', 'sentiment_score'].tolist()\n",
    "\n",
    "# Print the results for verification\n",
    "print(f\"Number of Positive Scores: {len(positive_scores)}\")\n",
    "print(f\"Number of Neutral Scores: {len(neutral_scores)}\")\n",
    "print(f\"Number of Negative Scores: {len(negative_scores)}\")"
   ],
   "id": "4f2f052753fb2e2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Positive Scores: 39298\n",
      "Number of Neutral Scores: 17935\n",
      "Number of Negative Scores: 13027\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Apply K-Means to Each Group",
   "id": "248cd9159c193003"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T11:05:07.036472Z",
     "start_time": "2025-01-19T11:04:51.219685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to apply K-Means clustering\n",
    "def apply_kmeans(scores, n_clusters=3):\n",
    "    scores_array = np.array(scores).reshape(-1, 1)  # Reshape for K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)  # Initialize K-Means\n",
    "    clusters = kmeans.fit_predict(scores_array)  # Fit and predict clusters\n",
    "    return clusters, kmeans.cluster_centers_\n",
    "\n",
    "# Apply K-Means to each sentiment group\n",
    "positive_clusters, positive_centers = apply_kmeans(positive_scores)\n",
    "neutral_clusters, neutral_centers = apply_kmeans(neutral_scores)\n",
    "negative_clusters, negative_centers = apply_kmeans(negative_scores)\n",
    "\n",
    "# Display cluster centers for verification\n",
    "print(f\"Positive Cluster Centers: {sorted(positive_centers.flatten())}\")\n",
    "print(f\"Neutral Cluster Centers: {sorted(neutral_centers.flatten())}\")\n",
    "print(f\"Negative Cluster Centers: {sorted(negative_centers.flatten())}\")\n"
   ],
   "id": "d365c12c15be8cb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Cluster Centers: [np.float64(0.5943230856781043), np.float64(0.8043295104286502), np.float64(0.9542473458008959)]\n",
      "Neutral Cluster Centers: [np.float64(0.5408110465505729), np.float64(0.6979260955658485), np.float64(0.8720885476951947)]\n",
      "Negative Cluster Centers: [np.float64(0.5576972346155221), np.float64(0.7310676325062735), np.float64(0.8803989664225644)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Combine Sentiment Label with Strength",
   "id": "5390dc35d1204dc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T11:45:50.896252Z",
     "start_time": "2025-01-19T11:45:50.843646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a function to map clusters to Weak, Mid, Strong\n",
    "def assign_cluster_labels(scores, clusters, centers):\n",
    "    # Map each cluster to Weak, Mid, or Strong based on sorted cluster centers\n",
    "    sorted_centers = sorted((val, idx) for idx, val in enumerate(centers.flatten()))\n",
    "    cluster_mapping = {sorted_centers[0][1]: \"Weak\",\n",
    "                       sorted_centers[1][1]: \"Mid\",\n",
    "                       sorted_centers[2][1]: \"Strong\"}\n",
    "\n",
    "    # Map cluster assignments to labels\n",
    "    cluster_labels = [cluster_mapping[c] for c in clusters]\n",
    "    return cluster_labels\n",
    "\n",
    "# Assign labels for each sentiment group\n",
    "positive_labels = assign_cluster_labels(positive_scores, positive_clusters, positive_centers)\n",
    "neutral_labels = assign_cluster_labels(neutral_scores, neutral_clusters, neutral_centers)\n",
    "negative_labels = assign_cluster_labels(negative_scores, negative_clusters, negative_centers)\n",
    "\n",
    "# Add cluster labels back to the original DataFrame\n",
    "df['strength'] = None  # Initialize the 'strength' column\n",
    "\n",
    "df.loc[df['sentiment_label'] == 'positive', 'strength'] = positive_labels\n",
    "df.loc[df['sentiment_label'] == 'neutral', 'strength'] = neutral_labels\n",
    "df.loc[df['sentiment_label'] == 'negative', 'strength'] = negative_labels\n",
    "\n",
    "# Display a few rows to verify\n",
    "print(\"Updated DataFrame with strength labels:\")\n",
    "display(df[['text', 'sentiment_label', 'sentiment_score', 'strength']].head())\n"
   ],
   "id": "f01456e5612d14ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame with strength labels:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                text sentiment_label  \\\n",
       "0  Running a business means juggling countless ad...        positive   \n",
       "1  Liz Truss is walking in the lingering shadow o...        negative   \n",
       "2  The UK is bracing for war as government buildi...        negative   \n",
       "3  Marrying a second or third cousin once removed...        positive   \n",
       "4  It's truly disgraceful how the Indian National...        negative   \n",
       "\n",
       "   sentiment_score strength  \n",
       "0         0.781216      Mid  \n",
       "1         0.882852   Strong  \n",
       "2         0.551017     Weak  \n",
       "3         0.747033      Mid  \n",
       "4         0.943839   Strong  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Running a business means juggling countless ad...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.781216</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liz Truss is walking in the lingering shadow o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.882852</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The UK is bracing for war as government buildi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.551017</td>\n",
       "      <td>Weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marrying a second or third cousin once removed...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.747033</td>\n",
       "      <td>Mid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's truly disgraceful how the Indian National...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.943839</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:01:41.936528Z",
     "start_time": "2025-01-19T16:01:41.627453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_csv_path = \"../data/strength_dataset.csv\"  # Specify the desired file path\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df.to_csv(output_csv_path, index=False)"
   ],
   "id": "fc506357d9fd0746",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classyfiying the texts into topics",
   "id": "32bbc379e0839cc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T19:13:32.321530Z",
     "start_time": "2025-01-19T16:02:14.127117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"../data/strength_dataset.csv\")\n",
    "\n",
    "# Initialize the topic classification pipeline\n",
    "topic_pipeline = pipeline(\"text-classification\", model=\"cardiffnlp/tweet-topic-21-multi\", truncation=True)\n",
    "\n",
    "# Classify tweets into topics using the topic classification pipeline\n",
    "df['topic'] = df['text'].apply(lambda x: topic_pipeline(x)[0]['label'])"
   ],
   "id": "208117cc7e40a308",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T19:20:03.907931Z",
     "start_time": "2025-01-19T19:20:03.502331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the updated DataFrame with the topic labels to a CSV file\n",
    "output_csv_path = \"../data/sentiment_analysis_with_topics.csv\"  # Specify the output file path\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_csv_path, index=False)"
   ],
   "id": "78ba19020f4b5747",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Visualize the Results",
   "id": "4209e7848b045ff5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2425df2664807c4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Hypothesis",
   "id": "72f6c872b9d2e2b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Users with extreme sentiments (strongly positive or negative) occupy central positions in the network.",
   "id": "14e6f52d15b4f712"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T15:15:06.060225Z",
     "start_time": "2025-01-19T13:54:34.302243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Load the social network edge list\n",
    "graph_df = pd.read_csv(\"../data/graph.csv\")\n",
    "\n",
    "# Create a graph using NetworkX\n",
    "G = nx.from_pandas_edgelist(graph_df, source='source', target='target', edge_attr='weight', create_using=nx.Graph())\n",
    "\n",
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)\n",
    "\n",
    "# Convert centrality dictionaries to DataFrame\n",
    "centrality_df = pd.DataFrame({\n",
    "    'user_id': degree_centrality.keys(),\n",
    "    'degree_centrality': degree_centrality.values(),\n",
    "    'betweenness_centrality': betweenness_centrality.values(),\n",
    "    'eigenvector_centrality': eigenvector_centrality.values()\n",
    "})\n",
    "\n",
    "# Load the sentiment analysis results\n",
    "sentiment_df = pd.read_csv(\"../data/sentiment_analysis_results.csv\")\n",
    "\n",
    "# Merge centrality data with sentiment data\n",
    "combined_df = pd.merge(sentiment_df, centrality_df, on='user_id', how='inner')\n",
    "\n",
    "# Display the combined dataset\n",
    "print(\"Combined dataset for hypothesis testing:\")\n",
    "display(combined_df.head())\n"
   ],
   "id": "771268272386f1dd",
   "outputs": [
    {
     "ename": "PowerIterationFailedConvergence",
     "evalue": "(PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPowerIterationFailedConvergence\u001B[0m           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m degree_centrality \u001B[38;5;241m=\u001B[39m nx\u001B[38;5;241m.\u001B[39mdegree_centrality(G)\n\u001B[1;32m     12\u001B[0m betweenness_centrality \u001B[38;5;241m=\u001B[39m nx\u001B[38;5;241m.\u001B[39mbetweenness_centrality(G)\n\u001B[0;32m---> 13\u001B[0m eigenvector_centrality \u001B[38;5;241m=\u001B[39m \u001B[43mnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meigenvector_centrality\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Convert centrality dictionaries to DataFrame\u001B[39;00m\n\u001B[1;32m     16\u001B[0m centrality_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser_id\u001B[39m\u001B[38;5;124m'\u001B[39m: degree_centrality\u001B[38;5;241m.\u001B[39mkeys(),\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdegree_centrality\u001B[39m\u001B[38;5;124m'\u001B[39m: degree_centrality\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetweenness_centrality\u001B[39m\u001B[38;5;124m'\u001B[39m: betweenness_centrality\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meigenvector_centrality\u001B[39m\u001B[38;5;124m'\u001B[39m: eigenvector_centrality\u001B[38;5;241m.\u001B[39mvalues()\n\u001B[1;32m     21\u001B[0m })\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/social-network-analysis-da1-KfAxiyL9-py3.12/lib/python3.12/site-packages/networkx/utils/decorators.py:788\u001B[0m, in \u001B[0;36margmap.__call__.<locals>.func\u001B[0;34m(_argmap__wrapper, *args, **kwargs)\u001B[0m\n\u001B[1;32m    787\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfunc\u001B[39m(\u001B[38;5;241m*\u001B[39margs, __wrapper\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 788\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43margmap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lazy_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m__wrapper\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<class 'networkx.utils.decorators.argmap'> compilation 26:4\u001B[0m, in \u001B[0;36margmap_eigenvector_centrality_22\u001B[0;34m(G, max_iter, tol, nstart, weight, backend, **backend_kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcollections\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgzip\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mre\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/social-network-analysis-da1-KfAxiyL9-py3.12/lib/python3.12/site-packages/networkx/utils/backends.py:967\u001B[0m, in \u001B[0;36m_dispatchable.__call__\u001B[0;34m(self, backend, *args, **kwargs)\u001B[0m\n\u001B[1;32m    965\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m backend \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m backend \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnetworkx\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    966\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbackend\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m backend is not installed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 967\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morig_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    969\u001B[0m \u001B[38;5;66;03m# Use `backend_name` in this function instead of `backend`.\u001B[39;00m\n\u001B[1;32m    970\u001B[0m \u001B[38;5;66;03m# This is purely for aesthetics and to make it easier to search for this\u001B[39;00m\n\u001B[1;32m    971\u001B[0m \u001B[38;5;66;03m# variable since \"backend\" is used in many comments and log/error messages.\u001B[39;00m\n\u001B[1;32m    972\u001B[0m backend_name \u001B[38;5;241m=\u001B[39m backend\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/social-network-analysis-da1-KfAxiyL9-py3.12/lib/python3.12/site-packages/networkx/algorithms/centrality/eigenvector.py:194\u001B[0m, in \u001B[0;36meigenvector_centrality\u001B[0;34m(G, max_iter, tol, nstart, weight)\u001B[0m\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28mabs\u001B[39m(x[n] \u001B[38;5;241m-\u001B[39m xlast[n]) \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m x) \u001B[38;5;241m<\u001B[39m nnodes \u001B[38;5;241m*\u001B[39m tol:\n\u001B[1;32m    193\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m x\n\u001B[0;32m--> 194\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m nx\u001B[38;5;241m.\u001B[39mPowerIterationFailedConvergence(max_iter)\n",
      "\u001B[0;31mPowerIterationFailedConvergence\u001B[0m: (PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_csv_path = \"../data/combined_dataset.csv\"\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "combined_df.to_csv(output_csv_path, index=False)"
   ],
   "id": "47ff2132b8f65a63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter for strong sentiments\n",
    "extreme_sentiments = combined_df[\n",
    "    (combined_df['strength'] == 'Strong') &\n",
    "    (combined_df['sentiment_label'].isin(['positive', 'negative']))\n",
    "]\n",
    "\n",
    "# Display the filtered dataset\n",
    "print(\"Filtered dataset with extreme sentiments:\")\n",
    "display(extreme_sentiments.head())\n"
   ],
   "id": "b426eac83f767691"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Correlate degree centrality with extreme sentiment\n",
    "pearson_degree, _ = pearsonr(extreme_sentiments['degree_centrality'], extreme_sentiments['sentiment_score'])\n",
    "spearman_degree, _ = spearmanr(extreme_sentiments['degree_centrality'], extreme_sentiments['sentiment_score'])\n",
    "\n",
    "# Correlate eigenvector centrality with extreme sentiment\n",
    "pearson_eigen, _ = pearsonr(extreme_sentiments['eigenvector_centrality'], extreme_sentiments['sentiment_score'])\n",
    "spearman_eigen, _ = spearmanr(extreme_sentiments['eigenvector_centrality'], extreme_sentiments['sentiment_score'])\n",
    "\n",
    "# Print correlation results\n",
    "print(f\"Pearson Correlation (Degree Centrality): {pearson_degree}\")\n",
    "print(f\"Spearman Correlation (Degree Centrality): {spearman_degree}\")\n",
    "print(f\"Pearson Correlation (Eigenvector Centrality): {pearson_eigen}\")\n",
    "print(f\"Spearman Correlation (Eigenvector Centrality): {spearman_eigen}\")\n"
   ],
   "id": "c2064840fc63e5fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
